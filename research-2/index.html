<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Research - Sylvain Argentieri</title><meta name="description" content="Interactive perception ... this is where we deal with what’s hidden in the sensorimotor flow! ... this is where we&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://my.sylar.org/research-2/"><link rel="alternate" type="application/atom+xml" href="https://my.sylar.org/feed.xml"><link rel="alternate" type="application/json" href="https://my.sylar.org/feed.json"><meta property="og:title" content="Research"><meta property="og:site_name" content="Sylvain Argentieri"><meta property="og:description" content="Interactive perception ... this is where we deal with what’s hidden in the sensorimotor flow! ... this is where we&hellip;"><meta property="og:url" content="https://my.sylar.org/research-2/"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://my.sylar.org/media/website/favicon.png" type="image/png"><link rel="stylesheet" href="https://my.sylar.org/assets/css/fontawesome-all.min.css?v=bbcde81f26378440dac4c3d195714389"><link rel="stylesheet" href="https://my.sylar.org/assets/css/style.css?v=c7cc6cb62f6909390827251ba7160305"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://my.sylar.org/research-2/"},"headline":"Research","datePublished":"2022-02-19T22:39","dateModified":"2022-02-20T18:40","description":"Interactive perception ... this is where we deal with what’s hidden in the sensorimotor flow! ... this is where we&hellip;","author":{"@type":"Person","name":"Sylvain Argentieri","url":"https://my.sylar.org/authors/sylvain-argentieri/"},"publisher":{"@type":"Organization","name":"Sylvain Argentieri"}}</script></head><body class="is-preload"><div id="wrapper"><div id="main"><div class="inner"><header id="header"><a class="logo" href="https://my.sylar.org/"><strong>Sylvain Argentieri</strong></a></header><article class="post"><header class="main post__header"><h1>Research</h1></header><div class="post__inner post__entry"><h2><a href="https://my.sylar.org/interactive-perception/">Interactive perception</a></h2><blockquote><p><a href="https://my.sylar.org/interactive-perception/">...  this is where we deal with what’s hidden in the sensorimotor flow!</a></p></blockquote><figure class="post__image post__image--center"><a href="https://my.sylar.org/interactive-perception/"><img loading="lazy" src="https://my.sylar.org/media/posts/13//featured_hu1f9c18600ed1f34dbab83b4d59492802_159577_540x0_resize_lanczos_2.png" alt="" width="540" height="196" sizes="(max-width: 48em) 100vw, 768px" srcset="https://my.sylar.org/media/posts/13//responsive/featured_hu1f9c18600ed1f34dbab83b4d59492802_159577_540x0_resize_lanczos_2-xs.png 300w, https://my.sylar.org/media/posts/13//responsive/featured_hu1f9c18600ed1f34dbab83b4d59492802_159577_540x0_resize_lanczos_2-sm.png 480w, https://my.sylar.org/media/posts/13//responsive/featured_hu1f9c18600ed1f34dbab83b4d59492802_159577_540x0_resize_lanczos_2-md.png 768w"></a></figure><p></p><h3 class="align-left">Selected publication</h3><table style="border-collapse: collapse; width: 100%;" border="1"><tbody><tr><td style="width: 100%;"><em data-darkreader-inline-color="">A Formal Account of Structuring Motor Actions With Sensory Prediction for a Naive Agent</em><span data-darkreader-inline-color="">.<br><span class="article-metadata li-cite-author" data-darkreader-inline-color="">Jean-Merwan Godon, Sylvain Argentieri, Bruno Gas </span>(2020). in Frontiers in Robotics and AI.<br><a href="file:///Users/sylar/Documents/Publii/sites/sylvain-argentieri/preview/Godon2020.pdf" target="_blank" rel="noopener noreferrer">📝PDF</a>  <a href="https://doi.org/10.3389/frobt.2020.561660" target="_blank" rel="noopener noreferrer">🌐DOI</a>  <a href="file:///Users/sylar/Documents/Publii/sites/sylvain-argentieri/preview/Godon2020.bib" target="_blank" rel="noopener noreferrer">📚Bibtex</a><strong><br></strong><br><figure class="post__image"><img loading="lazy" src="https://my.sylar.org/media/posts/13/featured_hu1d795fff3ac0b295b224327d906671a6_439544_918x517_fill_q90_lanczos_smart1_2.png" alt="" width="918" height="517" sizes="(max-width: 48em) 100vw, 768px" srcset="https://my.sylar.org/media/posts/13/responsive/featured_hu1d795fff3ac0b295b224327d906671a6_439544_918x517_fill_q90_lanczos_smart1_2-xs.png 300w, https://my.sylar.org/media/posts/13/responsive/featured_hu1d795fff3ac0b295b224327d906671a6_439544_918x517_fill_q90_lanczos_smart1_2-sm.png 480w, https://my.sylar.org/media/posts/13/responsive/featured_hu1d795fff3ac0b295b224327d906671a6_439544_918x517_fill_q90_lanczos_smart1_2-md.png 768w"></figure><br>For naive robots to become truly autonomous, they need a means of developing their perceptive capabilities instead of relying on hand crafted models. The sensorimotor contingency theory asserts that such a way resides in learning invariants of the sensorimotor flow. We propose a formal framework inspired by this theory for the description of sensorimotor experiences of a naive agent, extending previous related works. We then use said formalism to conduct a theoretical study where we isolate sufficient conditions for the determination of a sensory prediction function. Furthermore, we also show that algebraic structure found in this prediction can be taken as a proxy for structure on the motor displacements, allowing for the discovery of the combinatorial structure of said displacements. Both these claims are further illustrated in simulations where a toy naive agent determines the sensory predictions of its spatial displacements from its uninterpreted sensory flow, which it then uses to infer the combinatorics of said displacements.</span></td></tr></tbody></table><hr><h2><a href="https://my.sylar.org/robot-audition/">Robot Audition</a></h2><blockquote><p><a href="https://my.sylar.org/robot-audition/" target="_blank" rel="noopener noreferrer">... this is where we deal with how to understand an audio scene!</a></p></blockquote><figure class="post__image post__image--center"><a href="https://my.sylar.org/robot-audition/" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://my.sylar.org/media/posts/13/featured_huaca8b91f13bc14b487d5df59079fe7d0_147338_720x0_resize_q90_lanczos.jpg" alt="" width="720" height="240" sizes="(max-width: 48em) 100vw, 768px" srcset="https://my.sylar.org/media/posts/13/responsive/featured_huaca8b91f13bc14b487d5df59079fe7d0_147338_720x0_resize_q90_lanczos-xs.jpg 300w, https://my.sylar.org/media/posts/13/responsive/featured_huaca8b91f13bc14b487d5df59079fe7d0_147338_720x0_resize_q90_lanczos-sm.jpg 480w, https://my.sylar.org/media/posts/13/responsive/featured_huaca8b91f13bc14b487d5df59079fe7d0_147338_720x0_resize_q90_lanczos-md.jpg 768w"></a></figure><p></p><h3>Selected publication</h3><table style="border-collapse: collapse; width: 100%;" border="1"><tbody><tr><td style="width: 100%;"><strong><em>Binaural Localization of Multiple Sound Sources by Non-Negative Tensor Factorization<br></em></strong><span class="article-metadata li-cite-author">Elie Laurent Benaroya, Nicolas Obin, Marco Liuni, Axel Roebel, Wilson Raumel, Sylvain Argentieri (2018). in IEEE/ACM Transactions on Audio, Speech, and Language Processing.<br><a href="file:///Users/sylar/Documents/Publii/sites/sylvain-argentieri/preview/Benaroya2018.pdf" target="_blank" rel="noopener noreferrer">📝PDF</a>  <a href="https://doi.org/10.1109/TASLP.2018.2806745" target="_blank" rel="noopener noreferrer">🌐DOI</a>  <a href="file:///Users/sylar/Documents/Publii/sites/sylvain-argentieri/preview/Benaroya2018.bib" target="_blank" rel="noopener noreferrer">📚Bibtex</a><br><figure class="post__image post__image--center"><img loading="lazy" src="https://my.sylar.org/media/posts/13/Beranoya-2.png" alt="" width="706" height="410" sizes="(max-width: 48em) 100vw, 768px" srcset="https://my.sylar.org/media/posts/13/responsive/Beranoya-2-xs.png 300w, https://my.sylar.org/media/posts/13/responsive/Beranoya-2-sm.png 480w, https://my.sylar.org/media/posts/13/responsive/Beranoya-2-md.png 768w"></figure><span dir="ltr" role="presentation">This paper presents non-negative factorization of </span><span dir="ltr" role="presentation">audio signals for the binaural localization of multiple sound </span><span dir="ltr" role="presentation">sources within realistic and unknown sound environments. Non-</span><span dir="ltr" role="presentation">negative tensor factorization (NTF) provides a sparse represen</span><span dir="ltr" role="presentation">tation of multi-channel audio signals in time, frequency, and </span><span dir="ltr" role="presentation">space that can be exploited in computational audio scene analysis </span><span dir="ltr" role="presentation">and robot audition for the separation and localization of sound </span><span dir="ltr" role="presentation">sources.</span> <span dir="ltr" role="presentation">In</span> <span dir="ltr" role="presentation">the</span> <span dir="ltr" role="presentation">proposed</span> <span dir="ltr" role="presentation">formulation,</span> <span dir="ltr" role="presentation">each</span> <span dir="ltr" role="presentation">sound</span> <span dir="ltr" role="presentation">source</span> <span dir="ltr" role="presentation">is </span><span dir="ltr" role="presentation">represented by mean of spectral dictionaries, temporal activation,</span><br role="presentation"><span dir="ltr" role="presentation">and its distribution within each channel (here, left and right </span><span dir="ltr" role="presentation">ears). This distribution, being dependent on the frequency, can </span><span dir="ltr" role="presentation">be interpreted as an explicit estimation of the Head-Related </span><span dir="ltr" role="presentation">Transfer Function (HRTF) of a binaural head which can then</span><br role="presentation"><span dir="ltr" role="presentation">be converted into the estimated sound source position. Moreover, </span><span dir="ltr" role="presentation">the semi-supervised formulation of the non-negative factorization </span><span dir="ltr" role="presentation">allows to integrate prior knowledge about some sound sources of </span><span dir="ltr" role="presentation">interest whose dictionaries can be learned in advance, whereas </span><span dir="ltr" role="presentation">the remaining sources are considered as background sound which </span><span dir="ltr" role="presentation">remains unknown and is estimated on-the-fly. The proposed NTF-</span><br role="presentation"><span dir="ltr" role="presentation">based sound source localization is here applied to binaural sound </span><span dir="ltr" role="presentation">source localization of multiple speakers within realistic sound </span><span dir="ltr" role="presentation">environments.</span></span><em><br></em></td></tr></tbody></table></div><footer class="post__inner post__footer"><p class="post__last-updated">This article was updated on February 20, 2022</p></footer></article></div></div><div id="sidebar"><div class="inner"><nav id="menu"><header class="major"><h2>Menu</h2></header><ul><li><a href="https://my.sylar.org/biography/" target="_self">Home</a></li><li class="active-parent has-submenu"><a href="https://my.sylar.org/research-2/" target="_self" aria-haspopup="true" class="opener">Research</a><ul class="submenu level-2" aria-hidden="true"><li class="active"><a href="https://my.sylar.org/research-2/" target="_self">Overview</a></li><li><a href="https://my.sylar.org/interactive-perception/" target="_self">Interactive perception</a></li><li><a href="https://my.sylar.org/robot-audition/" target="_self">Robot audition</a></li></ul></li><li><a href="https://my.sylar.org/publications/" target="_self">Publications</a></li><li><a href="https://my.sylar.org/people/" target="_self">People</a></li><li class="has-submenu"><a href="https://my.sylar.org/overview/" target="_self" aria-haspopup="true" class="opener">Teaching</a><ul class="submenu level-2" aria-hidden="true"><li><a href="https://my.sylar.org/overview/" target="_self">Overview</a></li><li><a href="https://my.sylar.org/digital-signal-processing/" target="_self">Digital signal processing</a></li><li><a href="https://my.sylar.org/overview/" target="_self">ROS and experimental robotics</a></li><li><a href="https://my.sylar.org/overview/" target="_self">Sound source localization</a></li><li><a href="https://my.sylar.org/overview/" target="_self">Sensorimotor approaches to perception</a></li></ul></li><li><a href="https://my.sylar.org/tools/" target="_self">Tools</a></li><li><a href="https://my.sylar.org/stage-internship/" target="_self">Internship offer</a></li></ul></nav><section><header class="major"><h2>Get in touch</h2></header><p>You can find me <a href="https://osm.org/go/0BOd4gUuH?m=" target="_blank" rel="noopener">at ISIR</a>, office J02, or in the Master department, <a href="https://osm.org/go/0BOd4ggYU?m=">Esclangon building</a>, 2nd floor, office 219 (Faculty of Science &amp; Engineering, Sorbonne Université).</p><ul class="contact"><li class="icon solid fa-envelope"><a href="mailto:sylvain.argentieri_AT_sorbonne-universite.fr">my email</a></li><li class="icon solid fa-phone">(+33) 1 44 27 63 55</li><li class="icon solid fa-home">Sorbonne Université<br>Pyramide ISIR - T55/65, bureau J02<br>4 place Jussieu<br>75005 Paris, France</li></ul></section><footer id="footer"><p class="copyright">© Sylvain Argentieri - All rights reserved<a href="https://getpublii.com" target="_blank" rel="noopener"></a></p></footer></div></div></div><script src="https://my.sylar.org/assets/js/jquery.min.js?v=7c14a783dfeb3d238ccd3edd840d82ee"></script><script src="https://my.sylar.org/assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="https://my.sylar.org/assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="https://my.sylar.org/assets/js/util.min.js?v=cbdaf7c20ac2883c77ae23acfbabd47e"></script><script src="https://my.sylar.org/assets/js/main.min.js?v=448f4c9d4b4c374766a1cb76c3d88048"></script><script>var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};</script></body></html>