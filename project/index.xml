<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Sylvain Argentieri</title>
    <link>https://my.sylar.org/project/</link>
      <atom:link href="https://my.sylar.org/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2020 17:27:40 +0100</lastBuildDate>
    <image>
      <url>https://my.sylar.org/images/icon_hu1a01b3fab95c353a9499cb6a8a91062a_16677_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://my.sylar.org/project/</link>
    </image>
    
    <item>
      <title>Interactive perception</title>
      <link>https://my.sylar.org/project/interactive_perception/</link>
      <pubDate>Wed, 01 Jan 2020 17:27:40 +0100</pubDate>
      <guid>https://my.sylar.org/project/interactive_perception/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;Most of the existing works dealing with robot perception (and this includes my own on robot audition!) rely on the traditional, somewhat historical, following scheme (a.k.a. &lt;strong&gt;the perceive/plan/act scheme&lt;/strong&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, the robot reads and understands its sensations on the basis on a priori models given by the engineers (for instance, auditory, camera, laser, etc. models, trying to make the bridge between the raw sensors outputs and its high level interpretation)&lt;/li&gt;
&lt;li&gt;then, the system plan its action inside a generally well-known environment, whose characteristics are often identified beforehand&lt;/li&gt;
&lt;li&gt;finally, the action is actually performed by controlling the joints thanks to geometric/cinematic/dynamic models and controllers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://my.sylar.org/img/projects/sense-plan-act.png&#34; alt=&#34;The sense/plan/act paradigm&#34;&gt;&lt;/p&gt;
&lt;p&gt;This very generic approach do work pretty well! As long as your models (of the robot, its sensors, the world, etc.) do fit pretty well the reality for the given task, quite amazing results can be obtained. But being able to deal with incomplete models (&lt;em&gt;and they always are incomplete!&lt;/em&gt;), being able to face unpredictable situations where all your models can no longer apply is still a very difficult task. One solution could consist in questioning the aforementioned perception architecture. One could indeed envisage a new way to deal with perception where this ability is no longer somewhat given a priori by the designer of the system, but instead built and discovered by the robot, not from the raw sensory signals (i.e. the outputs od its sensors), but from the sensorimotor flow (i.e. the data made of the data coming from extero and/or proprioceptive sensors, and of the commands of the joints). Then, the previous architecture turns into&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://my.sylar.org/img/projects/sensorimotor-approach.png&#34; alt=&#34;The sensorimotor version of perception&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this line of research, the question of the emergence of internal representation of the robot interaction in its own environment is a central issue. It is indeed only on the basis on these representations that the robot will be able to plan and act in its environment (and not on &lt;em&gt;a priori&lt;/em&gt; models like in previous approaches). This question is absolutely fundamental when trying to build robot with autonomy and adaptability capabilities, which are mandatory capabilities in modern application of Robotics.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I chose to work on those topics through the &lt;em&gt;sensorimotor contingencies theory&lt;/em&gt;, that explains that perceiving is analog to the discovering of stable relations (i.e. &lt;em&gt;contingencies&lt;/em&gt;) linking motors and sensory events. Then characterizing these invariants (that is, determining precisely &lt;strong&gt;what they are&lt;/strong&gt; and &lt;strong&gt;what are their properties&lt;/strong&gt;) might help to investigate how internal representations built thanks to them could allow a robot to infer spatial structures without almost no a priori. For that purpose, we already tackled some of these aspects through two main contributions, mainly dealt with from a theoretical (mathematical) point of view:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://my.sylar.org/projects-details/dimension_estimation/&#34;&gt;space dimension estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://my.sylar.org/projects-details/structuration_sensorimotor_flow/&#34;&gt;structuration of the sensorimotor flow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Robot audition</title>
      <link>https://my.sylar.org/project/robot_audition/</link>
      <pubDate>Wed, 01 Jan 2020 17:20:14 +0100</pubDate>
      <guid>https://my.sylar.org/project/robot_audition/</guid>
      <description>&lt;p&gt;&lt;em&gt;“Blindness separate us from things but deafness from people”&lt;/em&gt; said Helen Keller, a famous American author who was the first deafblind person to obtain a Bachelor in Arts, in 1904. And indeed, being able to interpret an auditory scene from a robot is now a required capability when operating with humans, mainly interacting by speech with each others. Auditory scene analysis is a quite well-known research topic in Acoustics and Signal Processing, but considering its implication in Robotics, it is not as easy as expected to port all the already developments &lt;em&gt;inside&lt;/em&gt; a robot. The robotic context exhibit original constraints like embeddability, real-time, reveberations, ego-noise, etc.&lt;/p&gt;
&lt;p&gt;Two main paradigms are currently exploited in Robotics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on the one hand, &lt;strong&gt;array processing approaches&lt;/strong&gt; exploit microphones array to exploit redundant audio information to perform sound source localization, source separation, source recognition in a very efficient way. Recent developments definitely show that this is the way to go to develop an efficient audio system in Robotics;&lt;/li&gt;
&lt;li&gt;on the other hand, &lt;strong&gt;binaural approaches&lt;/strong&gt; try to somewhat mimic the human auditory system, at least from an external point of view (two ears, generally with two external ears). Using only two ears in a Robotics context is still a very challenging task. But while one could question the choice to restrict ourself to only two ears, this is also a unique opportunity to test auditory models of human audition, and to stress the importance of action in the hearing process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, hearing is rarely a purely static task. For instance, one often makes small head movements to disambiguate sound location, or better sound recognition. This is specifically what I have been dealing with, i.e. &lt;em&gt;&lt;strong&gt;active binaural audition&lt;/strong&gt;&lt;/em&gt;. On this topic, I have mainly proposed contributions on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://my.sylar.org/projects-details/binaural_cues_characterization/&#34;&gt;the characterization of binaural cues&lt;/a&gt; used for sound localization and source recognition in realistic conditions,&lt;/li&gt;
&lt;li&gt;the specific &lt;a href=&#34;https://my.sylar.org/projects-details/source_localization/&#34;&gt;sound localization&lt;/a&gt; problem,&lt;/li&gt;
&lt;li&gt;the use of the binaural movement to better sound localization,&lt;/li&gt;
&lt;li&gt;and the building of multimodal representation of unknown environments through head movements.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
